{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ],
      "metadata": {
        "id": "bWGi5rQBtqQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "Vm7cbBgofNdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/720Project/\""
      ],
      "metadata": {
        "id": "cSHSPf6K8r1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSx7M7bQ6oW1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import unquote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract hyperlinks from email body text\n",
        "def extract_hyperlinks(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    hyperlinks = []\n",
        "\n",
        "    # Pattern 1: Standard HTML hyperlinks <a href=\"http://...\">text</a>\n",
        "    href_pattern = re.compile(r'<a\\s+href=[\\'\\\"]?([^\\'\\\">]+)[\\'\\\"]?[^>]*>.*?</a>', re.IGNORECASE)\n",
        "    hyperlinks.extend(href_pattern.findall(text))\n",
        "\n",
        "    # Pattern 2: Plain URLs starting with http:// or https://\n",
        "    url_pattern = re.compile(r'(?:https?://|www\\.)[^\\s<>\"\\']+', re.IGNORECASE)\n",
        "    hyperlinks.extend(url_pattern.findall(text))\n",
        "\n",
        "    # Pattern 3: URLs in Yahoo Groups format (common in the provided examples)\n",
        "    yahoo_pattern = re.compile(r'http://(?:us\\.click\\.yahoo\\.com|docs\\.yahoo\\.com)[^\\s<>\"\\']+', re.IGNORECASE)\n",
        "    hyperlinks.extend(yahoo_pattern.findall(text))\n",
        "\n",
        "    # Pattern 4: URLs with line breaks or spaces\n",
        "    broken_url_pattern = re.compile(r'https?://[^\\s<>\"\\'\\(\\)]*(?:\\s+[^\\s<>\"\\'\\(\\)]+)*', re.IGNORECASE)\n",
        "    broken_urls = broken_url_pattern.findall(text)\n",
        "    for url in broken_urls:\n",
        "        if ' ' in url:  # Only process URLs that were broken with spaces\n",
        "            cleaned_url = url.replace(' ', '')\n",
        "            if cleaned_url not in hyperlinks:\n",
        "                hyperlinks.append(cleaned_url)\n",
        "\n",
        "    # Clean up URLs - remove any trailing punctuation or closing parentheses\n",
        "    cleaned_hyperlinks = []\n",
        "    for url in hyperlinks:\n",
        "        # Clean trailing punctuation\n",
        "        url = re.sub(r'[.,;:!?\"\\')]$', '', url)\n",
        "        # Decode URL-encoded characters\n",
        "        url = unquote(url)\n",
        "        # Remove any =3D encoding (common in email URLs)\n",
        "        url = url.replace('=3D', '=')\n",
        "        cleaned_hyperlinks.append(url)\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    unique_hyperlinks = []\n",
        "    for url in cleaned_hyperlinks:\n",
        "        if url not in unique_hyperlinks:\n",
        "            unique_hyperlinks.append(url)\n",
        "\n",
        "    return unique_hyperlinks\n"
      ],
      "metadata": {
        "id": "lnaeUCpedfPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset Into Training, Evaluation and Validation Sets\n",
        "# Training/Fine-tuning data sets= Ling, Nazarus, Nigerian; Evaluation= SpamAssassin; Validation=?\n",
        "\n",
        "# List of your CSV files\n",
        "csv_files = ['/content/drive/My Drive/720Project/SpamAssasin.csv', '/content/drive/My Drive/720Project/Nazario.csv', '/content/drive/My Drive/720Project/Nigerian_Fraud.csv']  # Replace with actual paths\n",
        "\n",
        "columns_to_use = ['sender', 'receiver', 'subject', 'body', 'label']\n",
        "\n",
        "df_list = [pd.read_csv(file, usecols=columns_to_use) for file in csv_files]\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Extract hyperlinks from the body text\n",
        "combined_df['hyperlinks'] = combined_df['body'].apply(extract_hyperlinks)"
      ],
      "metadata": {
        "id": "bJP5_7bEdkHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of hyperlinks per email\n",
        "combined_df['hyperlink_count'] = combined_df['hyperlinks'].apply(len)\n",
        "\n",
        "# Preview the result\n",
        "print(combined_df.head())\n",
        "print(f\"There are missing values?: {combined_df.isna().any().any()}\")\n",
        "print(f\"There are duplicated emails?: {combined_df['body'].duplicated().any()}\") # TODO: implement a better way of removing duplicates\n",
        "print(f\"There are empty strings in text column?: {combined_df.where(combined_df['body'] =='').any().any()}\")\n",
        "\n",
        "# Display some statistics about hyperlinks\n",
        "print(f\"\\nEmails containing hyperlinks: {sum(combined_df['hyperlink_count'] > 0)}\")\n",
        "print(f\"Total hyperlinks found: {combined_df['hyperlink_count'].sum()}\")\n",
        "print(f\"Maximum hyperlinks in a single email: {combined_df['hyperlink_count'].max()}\")"
      ],
      "metadata": {
        "id": "R3gpZxJxdxqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xzn1Gz5NtpHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune model"
      ],
      "metadata": {
        "id": "psMZZZWTtwhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "74sqSoCzvMlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0VQuSRKuoe5"
      },
      "outputs": [],
      "source": [
        "from transformers import (AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments,\n",
        "                          DataCollatorWithPadding, Trainer, pipeline)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch, wandb, evaluate\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = Dataset.from_pandas(combined_df)"
      ],
      "metadata": {
        "id": "hWo45iMzt1Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['hyperlinks'][1]"
      ],
      "metadata": {
        "id": "my5NuSViuSKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cybersectony/phishing-email-detection-distilbert_v2.4.1\")"
      ],
      "metadata": {
        "id": "YMsAiwr8vxXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data):\n",
        "    # # Extract selected features\n",
        "    # features = {\n",
        "    #     'sender': data['sender'],\n",
        "    #     'receiver': data['receiver'],\n",
        "    #     'hyperlinks': data['hyperlinks'],\n",
        "    #     'subject': data['subject'],\n",
        "    # }\n",
        "\n",
        "    # Tokenize email body (if still relevant)\n",
        "    body_tokens = tokenizer(str(data[\"body\"]),truncation=True,\n",
        "                       max_length=512, return_overflowing_tokens=True)\n",
        "\n",
        "    sample_map = body_tokens.pop(\"overflow_to_sample_mapping\")\n",
        "    for key, values in data.items():\n",
        "        body_tokens[key] = [values[i] for i in sample_map]\n",
        "    return body_tokens\n",
        "\n",
        "tokenized_dataset = dataset.map(prepare_data, batched=True)"
      ],
      "metadata": {
        "id": "jFUU085y5Vpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFb2hYPTx1N4"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define evaluation metrics"
      ],
      "metadata": {
        "id": "xWYsoeN8w62Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_9lbpvAyGA8"
      },
      "outputs": [],
      "source": [
        "metrics = evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"ealvaradob/false_positive_rate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soNxajXs3iWs"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  return metrics.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune model"
      ],
      "metadata": {
        "id": "izvJmVFnxH15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-08bi5s4f6o"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"benign\", 1: \"phishing\"}\n",
        "label2id = {\"benign\": 0, \"phishing\": 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5gAqybLp1-5"
      },
      "outputs": [],
      "source": [
        "df = tokenized_dataset.to_pandas()\n",
        "train, test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "train, test = Dataset.from_pandas(train, preserve_index=False), Dataset.from_pandas(test, preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6gUvTY1QYFL"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"cybersectony/phishing-email-detection-distilbert_v2.4.1\",\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2ejs78GQ_cc"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert-large-finetuned-phishing\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=4,\n",
        "    # torch_compile=True,\n",
        "    fp16=False,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "# # Evaluate on the evaluation dataset\n",
        "# results = trainer.evaluate()\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "YIYctQQRd3Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"bert-large-finetuned-phishing\")\n",
        "tokenizer.save_pretrained(\"bert-large-finetuned-phishing\")"
      ],
      "metadata": {
        "id": "ytg4cIOPiG5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r bert-large-finetuned-phishing.zip bert-large-finetuned-phishing\n",
        "from google.colab import files\n",
        "files.download(\"bert-large-finetuned-phishing.zip\")"
      ],
      "metadata": {
        "id": "owEQNI9hiI1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}